# DATA SCIENCE
> K-Nearest Neighbors (KNN)  - Modelos Supervisionados 06

![preview](/Knn.png)

### Projeto construido para complementar meu portif√≥lio de projetos em Data Science!  

Neste projeto de machine learning foi utilizado o algoritmo K-Nearest Neighbors (KNN), aplicado ao cl√°ssico dataset Iris. Esse conjunto de dados cont√©m informa√ß√µes sobre tr√™s esp√©cies de flores ‚Äì Setosa, Versicolor e Virginica ‚Äì com quatro caracter√≠sticas f√≠sicas: comprimento e largura de p√©talas e s√©palas. O objetivo do projeto foi criar um modelo capaz de classificar corretamente a esp√©cie de cada flor com base nessas caracter√≠sticas. Comecei explorando os dados e visualizando as rela√ß√µes entre as vari√°veis, o que ajudou a entender como as diferentes esp√©cies se distinguem entre si. Depois, realizei o pr√©-processamento dos dados, normalizando as vari√°veis para garantir que a escala n√£o afetasse a performance do modelo.

Apesar de ser um algoritmo poderoso em muitas aplica√ß√µes de classifica√ß√£o, o K-Nearest Neighbors (KNN) tem algumas limita√ß√µes importantes. Uma das principais desvantagens √© sua sensibilidade ao volume de dados, pois sua efici√™ncia diminui √† medida que o tamanho do conjunto de dados aumenta, j√° que o KNN precisa calcular a dist√¢ncia de cada novo ponto em rela√ß√£o a todos os pontos de treinamento. Al√©m disso, ele √© sens√≠vel √† presen√ßa de ru√≠do e outliers, j√° que pontos isolados podem influenciar negativamente as classifica√ß√µes. Outro ponto cr√≠tico √© a necessidade de normaliza√ß√£o dos dados, uma vez que o KNN se baseia na dist√¢ncia euclidiana, o que significa que vari√°veis em escalas diferentes podem distorcer os resultados.

Apesar dessas limita√ß√µes, o KNN tamb√©m pode ser utilizado de forma criativa, como no tratamento de valores faltantes em um conjunto de dados. Quando faltam valores em determinadas vari√°veis, o KNN pode ser aplicado para encontrar as observa√ß√µes mais pr√≥ximas com base nas outras vari√°veis conhecidas e, a partir dessas observa√ß√µes vizinhas, substituir os valores ausentes com a m√©dia, mediana ou moda dessas observa√ß√µes. Esse m√©todo √© particularmente √∫til quando a aus√™ncia de dados n√£o √© aleat√≥ria e existe uma rela√ß√£o clara entre as vari√°veis dispon√≠veis e os dados que est√£o faltando, aproveitando a simplicidade e flexibilidade do algoritmo para a imputa√ß√£o de valores faltantes.



## üîß Tecnologias üîß

- pandas 
- matplotlib.pyplot 
- seaborn 
- sklearn
    - K-Nearest Neighbors (KNN)
    - Metrics 
    - Train test split

## ‚ù§Ô∏è Contato ‚ù§Ô∏è
 
- Email: Rafaelssm7@gmail.com
- LinkedIN: https://www.linkedin.com/in/rssms/